import os
os.environ["DISABLE_BF16"] = "1"  # CUDA 11.8 å¿…é¡»

import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from trl import SFTTrainer


# =========================
# 1. åŸºæœ¬é…ç½®
# =========================
model_name = "Qwen/Qwen2.5-7B-Instruct"
data_path = "fundus_finetune.jsonl"
output_dir = "./fundus_lora"

MAX_LEN = 1024


# =========================
# 2. åŠ è½½æ•°æ®
# =========================
dataset = load_dataset(
    "json",
    data_files=data_path,
    split="train"
)


# =========================
# 3. Tokenizer
# =========================
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
tokenizer.model_max_length = MAX_LEN  # âš ï¸ è€ TRL åªèƒ½è¿™æ ·æ§é•¿åº¦


# =========================
# 4. QLoRA 4-bit é‡åŒ–é…ç½®
# =========================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)


# =========================
# 5. åŠ è½½æ¨¡å‹ï¼ˆ4bitï¼‰
# =========================
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

model = prepare_model_for_kbit_training(model)


# =========================
# 6. LoRA é…ç½®ï¼ˆQLoRA æ ¸å¿ƒï¼‰
# =========================
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()


# =========================
# 7. æ•°æ®æ ¼å¼åŒ–å‡½æ•°ï¼ˆâš ï¸ å¿…é¡»è¿”å› listï¼‰
# =========================
def formatting_func(example):
    text = (
        "### Question:\n"
        f"{example['question']}\n\n"
        "### Answer:\n"
        f"{example['answer']}"
    )
    return [text]   # âš ï¸ TRL 0.9.x å¼ºåˆ¶è¦æ±‚ list


# =========================
# 8. TrainingArguments
# =========================
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=2,      # 12G æ˜¾å­˜å®‰å…¨å€¼
    gradient_accumulation_steps=8,      # ç­‰æ•ˆ batch = 16
    learning_rate=2e-4,                 # QLoRA æ¨è
    num_train_epochs=3,
    logging_steps=5,
    save_strategy="epoch",
    fp16=True,
    bf16=False,
    optim="paged_adamw_8bit",            # çœæ˜¾å­˜å…³é”®
    report_to="none",
    dataloader_num_workers=4,
)


# =========================
# 9. SFTTrainerï¼ˆè€ç‰ˆæœ¬ç¨³å®šå†™æ³•ï¼‰
# =========================
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,          # âš ï¸ 0.9.6 ä»ç„¶éœ€è¦
    args=training_args,
    train_dataset=dataset,
    formatting_func=formatting_func,
)


# =========================
# 10. å¼€å§‹è®­ç»ƒ
# =========================
print("ğŸš€ å¼€å§‹ QLoRA å¾®è°ƒ ...")
trainer.train()


# =========================
# 11. ä¿å­˜ LoRA æƒé‡
# =========================
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

print(f"âœ… å¾®è°ƒå®Œæˆï¼ŒLoRA æƒé‡å·²ä¿å­˜è‡³ {output_dir}")
